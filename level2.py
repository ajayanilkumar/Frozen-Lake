# -*- coding: utf-8 -*-
"""Level2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1awvgt0nDWl0MC2Pl_X1va_ejRa2iwBe1

<a href="https://colab.research.google.com/github/tekgulburak/FrozenLake-v1/blob/main/FrozenLake_v1.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
"""

pip install gymnasium

!pip install matplotlib

import random
import numpy as np
import gymnasium as gym
import matplotlib.pyplot as plt
from IPython import display

!pip install gym==0.20.0.

import random

def generate_custom_map(size=8, num_holes=8):
    custom_map = ['F' * size for _ in range(size)]


    start_position = (0, 0)
    goal_position = (size - 1, size - 1)
    custom_map[start_position[0]] = custom_map[start_position[0]][:start_position[1]] + 'S' + custom_map[start_position[0]][start_position[1] + 1:]
    custom_map[goal_position[0]] = custom_map[goal_position[0]][:goal_position[1]] + 'G' + custom_map[goal_position[0]][goal_position[1] + 1:]


    hole_positions = set()
    while len(hole_positions) < num_holes:
        row = random.randint(0, size - 1)
        col = random.randint(0, size - 1)

        if (row, col) not in [start_position, goal_position] and (row, col) not in hole_positions:
            hole_positions.add((row, col))
            custom_map[row] = custom_map[row][:col] + 'H' + custom_map[row][col + 1:]

    return custom_map

custom_map = generate_custom_map()

env = gym.make("FrozenLake-v1", desc=custom_map, render_mode="rgb_array", is_slippery=False)

#pip install pygame

env.reset()
img=env.render()
plt.imshow(img)
plt.show()

n_states=env.observation_space.n
n_actions=env.action_space.n

print(f" num of states:{n_states}\n num of actions:{n_actions}")

state,info=env.reset()
img=plt.imshow(env.render())
while True:
    action = env.action_space.sample()
    state, reward, terminated, truncated, info = env.step(action)
    img.set_data(env.render())
    display.display(plt.gcf())
    display.clear_output(wait=True)

    if terminated:
            break

Q=np.zeros([n_states,n_actions])
Q.shape

Q

episodes=10000
alpha=0.5
gamma=0.9
G=0 #G is sum of rewards

for episode in range(1,episodes+1):
  state=env.reset()[0]
  done=False
  G=0
  while not done:

    if np.max(Q[state]) > 0:
        action = np.argmax(Q[state])


    else:
        action = env.action_space.sample()

    new_state,reward,done,info,a=env.step(action)
    Q[state,action]+=alpha*(reward+gamma*np.max(Q[new_state])-Q[state,action])
    G+=reward
    state=new_state
  if episode%100==0:
      print(f"episode {episode} sum of  reward :{G}")

Q

state=env.reset()[0]
done=False

while not done:
  if np.max(Q[state])>0:
    action=np.argmax(Q[state])

  else:
    action=env.action_space.sample()

  new_state,reward,done,info,x=env.step(action)
  img=env.render()
  plt.imshow(img)
  plt.show()
  state=new_state

reward

done

info

state=env.reset()[0]
done=False

while not done:
  if np.max(Q[state])>0:
    action=np.argmax(Q[state])

  else:
    action=env.action_space.sample()

  new_state,reward,done,info,x=env.step(action)

  img=plt.imshow(env.render())
  display.display(plt.gcf())
  display.clear_output(wait=True)

  state=new_state



